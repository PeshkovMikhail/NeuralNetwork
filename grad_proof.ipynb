{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from linear import *\n",
    "from convolutional import *\n",
    "\n",
    "def check_gradient(f, x, delta=1e-5, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Checks the implementation of analytical gradient by comparing\n",
    "    it to numerical gradient using two-point formula\n",
    "\n",
    "    Arguments:\n",
    "      f: function that receives x and computes value and gradient\n",
    "      x: np array, initial point where gradient is checked\n",
    "      delta: step to compute numerical gradient\n",
    "      tol: tolerance for comparing numerical and analytical gradient\n",
    "\n",
    "    Return:\n",
    "      bool indicating whether gradients match or not\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray)\n",
    "    assert x.dtype == np.float64\n",
    "\n",
    "    fx, analytic_grad = f(x)\n",
    "    analytic_grad = analytic_grad.copy()\n",
    "\n",
    "    assert analytic_grad.shape == x.shape\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        analytic_grad_at_ix = analytic_grad[ix]\n",
    "        numeric_grad_at_ix = 0\n",
    "        arr_delta = np.zeros(x.shape)\n",
    "        arr_delta[ix] = delta\n",
    "        numeric_grad_at_ix = ( f(x+arr_delta)[0] - f(x-arr_delta)[0] ) / (2*delta)\n",
    "        \n",
    "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
    "            print(\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\" % (\n",
    "                  ix, analytic_grad_at_ix, numeric_grad_at_ix))\n",
    "            return False\n",
    "\n",
    "        it.iternext()\n",
    "\n",
    "    print(\"Gradient check passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_layer_gradient(layer, x, delta=1e-5, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Checks gradient correctness for the input and output of a layer\n",
    "\n",
    "    Arguments:\n",
    "      layer: neural network layer, with forward and backward functions\n",
    "      x: starting point for layer input\n",
    "      delta: step to compute numerical gradient\n",
    "      tol: tolerance for comparing numerical and analytical gradient\n",
    "\n",
    "    Returns:\n",
    "      bool indicating whether gradients match or not\n",
    "    \"\"\"\n",
    "    output = layer.forward(x)\n",
    "    output_weight = np.random.randn(*output.shape)\n",
    "\n",
    "    def helper_func(x):\n",
    "        output = layer.forward(x)\n",
    "        loss = np.sum(output * output_weight)\n",
    "        d_out = np.ones_like(output) * output_weight\n",
    "        grad = layer.backward(d_out, 0)\n",
    "        return loss, grad\n",
    "\n",
    "    return check_gradient(helper_func, x, delta, tol)\n",
    "\n",
    "\n",
    "def check_layer_param_gradient(layer, x,\n",
    "                               param_name,\n",
    "                               delta=1e-5, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Checks gradient correctness for the parameter of the layer\n",
    "\n",
    "    Arguments:\n",
    "      layer: neural network layer, with forward and backward functions\n",
    "      x: starting point for layer input\n",
    "      param_name: name of the parameter\n",
    "      delta: step to compute numerical gradient\n",
    "      tol: tolerance for comparing numerical and analytical gradient\n",
    "\n",
    "    Returns:\n",
    "      bool indicating whether gradients match or not\n",
    "    \"\"\"\n",
    "    param = layer.params()[param_name]\n",
    "    initial_w = param.value\n",
    "\n",
    "    output = layer.forward(x)\n",
    "    output_weight = np.random.randn(*output.shape)\n",
    "\n",
    "    def helper_func(w):\n",
    "        param.value = w\n",
    "        output = layer.forward(x)\n",
    "        loss = np.sum(output * output_weight)\n",
    "        d_out = np.ones_like(output) * output_weight\n",
    "        layer.backward(d_out, 0)\n",
    "        grad = param.grad\n",
    "        return loss, grad\n",
    "\n",
    "    return check_gradient(helper_func, initial_w, delta, tol)\n",
    "        \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Computes gradient with respect to input and\n",
    "        accumulates gradients within self.W and self.B\n",
    "\n",
    "        Arguments:\n",
    "        d_out, np array (batch_size, n_output) - gradient\n",
    "           of loss function with respect to output\n",
    "\n",
    "        Returns:\n",
    "        d_result: np array (batch_size, n_input) - gradient\n",
    "          with respect to input\n",
    "        \"\"\"\n",
    "        batch_size = d_out.shape[0]\n",
    "        self.W.grad = np.dot(self.X.T, d_out)\n",
    "        self.B.grad = np.dot(np.ones((1, batch_size)), d_out)\n",
    "        d_input = np.dot(d_out, self.W.value.T)\n",
    "        return d_input\n",
    "\n",
    "    def params(self):\n",
    "        return {'W': self.W, 'B': self.B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_layer_param_gradient(Linear(5, 2, None), np.random.uniform(size=(1,5)), \"W\")\n",
    "check_layer_param_gradient(Linear(5, 2, None), np.random.uniform(size=(1,5)), \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_layer_param_gradient(ConvLayer(5, 2, 5), np.random.uniform(size=(1,5, 32, 32)), \"W\")\n",
    "check_layer_param_gradient(ConvLayer(5, 2, 5), np.random.uniform(size=(1,5, 32, 32)), \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_layer_param_gradient(ConvLayer(5, 2, 5, stride=2, padding=2), np.random.uniform(size=(1,5, 32, 32)), \"W\")\n",
    "check_layer_param_gradient(ConvLayer(5, 2, 5, stride=2, padding=2), np.random.uniform(size=(1,5, 32, 32)), \"B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
